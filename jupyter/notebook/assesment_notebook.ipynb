{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PySpark in Jupyter Notebook and other environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%pip install findspark\n",
    "%pip install pyspark\n",
    "%pip install py4j\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all dependencies for the Spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "\n",
    "import findspark\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, max, min, collect_list\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"data_engineering_task\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Reading data and printing schema values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_file = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .load(\"/workspaces/data_engineering_task/dataset/nyc-jobs.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Remove duplicate rows\n",
    "print(\"count before remove duplicate\",data_file.count())\n",
    "#data_file = data_file.dropDuplicates()\n",
    "# Print the schema to understand column types\n",
    "print(\"count after removed duplicate\",data_file.count())\n",
    "# Print the schema to understand column types\n",
    "data_file.printSchema()\n",
    "# Show the first few rows of the DataFrame\n",
    "#data_file.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary statistics for numerical columns\n",
    "data_file.describe([\"# Of Positions\", \"Salary Range From\", \"Salary Range To\"]).show()\n",
    "\n",
    "# Count distinct values for categorical columns\n",
    "categorical_columns = [\"Agency\", \"Posting Type\", \"Title Code No\", \"Level\", \"Job Category\", \"Full-Time/Part-Time indicator\", \"Salary Frequency\"]\n",
    "\n",
    "for column in categorical_columns:\n",
    "    print(f\"Distinct values in {column}:\")\n",
    "    data_file.select(column).distinct().show(10, truncate=False)\n",
    "\n",
    "\n",
    "# Count null values in each column\n",
    "data_file.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in data_file.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#List of KPIs to be resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count the number of jobs per agency\n",
    "agency_counts = data_file.groupBy(\"Agency\").count().orderBy(\"count\", ascending=False)\n",
    "agency_counts.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.number of jobs posting per category top 10:\n",
    "job_category_counts = data_file.filter(col(\"Job Category\").isNotNull()).groupBy(\"Job Category\").count().orderBy(\"count\", ascending=False)\n",
    "job_category_counts.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. salary distribution per job category\n",
    "salary_distribution = data_file.groupBy(\"Job Category\").agg(sum(\"Salary Range From\").alias(\"Total Salary Range From\"),\n",
    "                                                            sum(\"Salary Range To\").alias(\"Total Salary Range To\"),\n",
    "                                                            sum(\"# Of Positions\").alias(\"Total Positions\")) \\\n",
    "    .orderBy(\"Total Salary Range From\", ascending=False)\n",
    "salary_distribution.show(10, truncate=False)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Correlation between the higher degree and the salary.\n",
    "\n",
    "data_with_degree = data_file.withColumn(\n",
    "    \"Degree Type\",\n",
    "    regexp_extract(col(\"Minimum Qual Requirements\"), r\"(\\w+)\\sdegree\", 1)\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "data_with_degree.select(\"Degree Type\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Job posting having the highest salary per agency\n",
    "highest_salary_per_agency = data_file.groupBy(\"Agency\").agg(\n",
    "    {\"Salary Range From\": \"max\", \"Salary Range To\": \"max\"}\n",
    ").orderBy(\"max(Salary Range To)\", ascending=False)\n",
    "highest_salary_per_agency.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Job positings average salary per agency for the last 2 years\n",
    "\n",
    "#finding most lasted year to find last 2 years data.\n",
    " \n",
    "find_year = data_file.withColumn(\"Year\", year(col(\"Posting Date\"))).distinct()\n",
    "current_year=find_year.filter(col(\"year\").isNotNull()).select(\"Year\").orderBy(\"Year\",ascending=False).first()[\"Year\"]\n",
    "\n",
    "\n",
    "# Filter data for the last 2 years\n",
    "filtered_data = data_file.filter(\n",
    "    (col(\"Posting Date\").isNotNull()) & \n",
    "    (year(col(\"Posting Date\")) >= (current_year - 2))\n",
    ")\n",
    "\n",
    "# Calculate average salary per agency\n",
    "average_salary_per_agency = filtered_data.filter(col(\"Posting Date\").isNotNull()) \\\n",
    "    .groupBy(\"Agency\",\"Posting Date\") \\\n",
    "    .agg({\"Salary Range From\": \"avg\", \"Salary Range To\": \"avg\"}) \\\n",
    "    .orderBy(\"avg(Salary Range To)\", ascending=False)\n",
    "average_salary_per_agency.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 highest paid skills in the US market\n",
    "\n",
    "def find_highest_paid_skills(df: DataFrame) -> DataFrame:\n",
    "    # Extract skills or keywords from the \"Job Description\" or \"Minimum Qual Requirements\" column\n",
    "    df_with_skills = df.withColumn(\n",
    "        \"Skill\",\n",
    "        regexp_extract(col(\"Minimum Qual Requirements\"), r\"(\\w+)\\s(skill|experience|knowledge)\", 1)\n",
    "    )\n",
    "    \n",
    "    # Filter rows where skills are identified\n",
    "    df_with_skills = df_with_skills.filter(col(\"Skill\").isNotNull())\n",
    "    \n",
    "    # Calculate the average salary for each skill\n",
    "    skill_salary = df_with_skills.groupBy(\"Skill\").agg(\n",
    "        avg(\"Salary Range From\").alias(\"Avg Salary Range From\"),\n",
    "        avg(\"Salary Range To\").alias(\"Avg Salary Range To\")\n",
    "    ).orderBy(\"Avg Salary Range To\", ascending=False)\n",
    "    \n",
    "    return skill_salary\n",
    "\n",
    "# Example usage\n",
    "highest_paid_skills = find_highest_paid_skills(data_file)\n",
    "highest_paid_skills.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    def clean_dataset(df: DataFrame) -> DataFrame:\n",
    "        print(f\"Count before removing duplicates: {df.count()}\")\n",
    "        df = df.dropDuplicates()\n",
    "        print(f\"Count after removing duplicates: {df.count()}\")\n",
    "        return df\n",
    "\n",
    "    # Function to preprocess columns (e.g., extract year, clean text)\n",
    "    def preprocess_columns(df: DataFrame) -> DataFrame:\n",
    "        df = df.withColumn(\"Year\", year(col(\"Posting Date\")))\n",
    "        df = df.withColumn(\n",
    "            \"Degree Type\",\n",
    "            regexp_extract(col(\"Minimum Qual Requirements\"), r\"(\\w+)\\sdegree\", 1)\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    # Function for data wrangling (e.g., filtering, grouping)\n",
    "    def wrangle_data(df: DataFrame, current_year: int) -> DataFrame:\n",
    "        # Filter data for the last 2 years\n",
    "        filtered_data = df.filter(\n",
    "            (col(\"Posting Date\").isNotNull()) & \n",
    "            (year(col(\"Posting Date\")) >= (current_year - 2))\n",
    "        )\n",
    "        return filtered_data\n",
    "\n",
    "    # Function for data transformation (e.g., aggregations, calculations)\n",
    "    def transform_data(df: DataFrame) -> DataFrame:\n",
    "        # Example: Calculate average salary per agency\n",
    "        transformed_df = df.groupBy(\"Agency\").agg(\n",
    "            sum(\"Salary Range From\").alias(\"Total Salary Range From\"),\n",
    "            sum(\"Salary Range To\").alias(\"Total Salary Range To\"),\n",
    "            sum(\"# Of Positions\").alias(\"Total Positions\")\n",
    "        ).orderBy(\"Total Salary Range From\", ascending=False)\n",
    "        return transformed_df\n",
    "\n",
    "    # Clean the dataset\n",
    "    data_file = clean_dataset(data_file)\n",
    "\n",
    "    # Preprocess columns\n",
    "    data_file = preprocess_columns(data_file)\n",
    "\n",
    "    # Find the current year\n",
    "    current_year = data_file.select(year(col(\"Posting Date\")).alias(\"Year\")).distinct().orderBy(col(\"Year\").desc()).first()[\"Year\"]\n",
    "\n",
    "    # Wrangle the data\n",
    "    filtered_data = wrangle_data(data_file, current_year)\n",
    "\n",
    "    # Transform the data\n",
    "    final_data = transform_data(filtered_data)\n",
    "\n",
    "    # Final transformed data to save as file.\n",
    "    final_data.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"/workspaces/data_engineering_task/dataset/final_transformed_data.csv\")\n",
    "\n",
    "    # final data to write to hive table (make sure final table exists).\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS sample_db\")\n",
    "    final_data.write.format(\"orc\").mode(\"overwrite\").saveAsTable(\"sample_db.final_transformed_data\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
